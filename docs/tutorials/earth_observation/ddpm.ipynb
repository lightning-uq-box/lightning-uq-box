{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probababilistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will illustrate how you can train a diffusion model on your dataset. Many downstream applications of diffusion models involve training an unconditional diffusion first, before using such a model in a specialized prediction scheme, such as inpainting, denoising etc. Therefore, we will first show how to train an unconditional model and in a separate tutorial available [here], you can see an example of inpainting that also aims to illustrate what role UQ can potentially play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "Numerous great introductions to diffusion models exist (linked below), so therefore we will only try to give a brief overview and intuition about this class of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchgeo.transforms import AugmentationSequential\n",
    "import kornia.augmentation as K\n",
    "from lightning import LightningDataModule, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VisionDataset\n",
    "from typing import Any\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "\n",
    "from denoising_diffusion_pytorch.denoising_diffusion_pytorch import Unet, GaussianDiffusion\n",
    "\n",
    "from lightning_uq_box.uq_methods import DDPM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Instead of implementing these models from scratch, we will utilize the implementations of this great [repo](https://github.com/lucidrains/denoising-diffusion-pytorch) by [lucidrains](https://lucidrains.github.io/), so please consider giving it a star to show your appreciation. The Lightning-UQ-Box provides LightningModule wrappers that execute the training via Lightning so you can benefit from all the multi-gpu and logging support that Lightning provides.\n",
    "\n",
    "For the model we need three parts:\n",
    "- the denoising model architecture, commonly a Unet\n",
    "- the diffusion model that can execute the denoising step as well as sampling\n",
    "- the diffusion LightningModule that can execute the training via lightning\n",
    "\n",
    "The first two components are implemented in [denoising-diffusion-pytorch](https://github.com/lucidrains/denoising-diffusion-pytorch) and the last one in the Lightning-UQ-Box.\n",
    "\n",
    "In addition, we need a LightningDataModule that loads the images for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = Unet(\n",
    "    dim=64,\n",
    "    dim_mults=[1, 2, 4, 8, 16, 32],\n",
    "    channels=3,\n",
    "    self_condition=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diffusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_model = GaussianDiffusion(\n",
    "    unet,\n",
    "    image_size= 224,\n",
    "    timesteps= 500,\n",
    "    sampling_timesteps=250,\n",
    "    beta_schedule=\"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "The more recent paper by [Karras et al 2022]() proposed some changes that can improve results. If you want to use that version, use `ElucidatedDiffusion` instead of `GaussianDiffusion` availabel as `from denoising_diffusion_pytorch.elucidated_diffusion import ElucidatedDiffusion`. For this implementation you also need to change the Unet architecture to either use `learned_sinusoidal_cond=True` or `random_fourier_features=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightningModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpm = DDPM(\n",
    "    diff_model,\n",
    "    log_samples_every_n_steps=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataModule\n",
    "\n",
    "For our purposes we will used the imagery of the [Million-Aid Dataset](https://captain-whu.github.io/DiRS/) that can also be loaded with [torchgeo](https://github.com/microsoft/torchgeo). The goal will be to train a model that can generate samples from the training data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MillionAidDataset(VisionDataset):\n",
    "    resize = K.Resize((224, 224))\n",
    "    def __init__(self, root: str):\n",
    "        super().__init__(root)\n",
    "\n",
    "        self.fpaths = sorted(glob(root + '/*.jpg', recursive=True))\n",
    "        assert len(self.fpaths) > 0, \"File list is empty. Check the root.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fpaths)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        fpath = self.fpaths[index]\n",
    "        img = Image.open(fpath).convert('RGB')\n",
    "\n",
    "\n",
    "        img = pil_to_tensor(img) / 255.\n",
    "        img = (img).squeeze(0)\n",
    "\n",
    "        return {\"input\": img}\n",
    "\n",
    "\n",
    "class MillionAIDDataModule(LightningDataModule):\n",
    "    \"\"\"DataModule for MillionAID dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root: str, batch_size: int = 64, num_workers: int = 4):\n",
    "        \"\"\"Initialize a new instance of MillionAIDDataModule.\n",
    "        \n",
    "        Args:\n",
    "            root: The root directory of the dataset.\n",
    "            batch_size: The batch size.\n",
    "            num_workers: The number of workers for the DataLoader.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.aug = AugmentationSequential(\n",
    "            K.RandomHorizontalFlip(p=0.5),\n",
    "            K.RandomVerticalFlip(p=0.5),\n",
    "            data_keys=[\"input\"],\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: str | None = None):\n",
    "        self.train = MillionAidDataset(root=self.root)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "    \n",
    "    def on_after_batch_transfer(self, batch: dict[str, Any], dataloader_idx: int) -> dict[str, Any]:\n",
    "        \"\"\"Apply augmentations to the batch.\"\"\"\n",
    "        return self.aug(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Training Diffusion Models is quiet computationally expensive. We have therefore pretrained a model to save a lot of time. However, the code below shows how you can train a model on your own dataset. In the included gif, you can see how the model capability to generate samples increases over training time, first random blobs and later samples that look like data points from our training distribution, namely the MillionAid dataset.\n",
    "\n",
    "TODO: Include GIF here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_CKPT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MillionAIDDataModule(root=\"/mnt/SSD2/nils/ocean_bench_exps/diffusion/data/million/test/test\", batch_size=64, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_CKPT:\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=\"ddp\",\n",
    "        accumulate_grad_batches=4,\n",
    "        devices=[4, 5, 6, 7],\n",
    "        min_epochs=800,\n",
    "        max_epochs=1000,\n",
    "        log_every_n_steps=50,\n",
    "        enable_progress_bar=True,\n",
    "        enable_checkpointing=True\n",
    "    )\n",
    "    trainer.fit(ddpm, dm)\n",
    "\n",
    "else:\n",
    "    # load checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "In the following sections we will generate some samples from the diffusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devuqbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
