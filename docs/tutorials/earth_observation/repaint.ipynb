{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting with RePaint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the many possible downstream applications of diffusion models is inpainting, the task of filling in missing data given some context. One approach to inpaiting has been proposed by [Lugmayr et al. 2022](https://arxiv.org/abs/2201.09865) with their method named RePaint. It proposed an adapted sampling strategy based on a standard diffusion model. If you are interested in how to train such models from scratch for new datasets, head over to this [tutorial]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Short introduction to RePaint. Maybe just include the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import kornia.augmentation as K\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from denoising_diffusion_pytorch.repaint import GaussianDiffusion as RePaint\n",
    "\n",
    "from torchgeo.datasets import MillionAID\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "The model has been pretrained on the [MillionAid Dataset](https://captain-whu.github.io/DiRS/) that we will load with [torchgeo](https://github.com/microsoft/torchgeo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MillionAID(root=\"/mnt/SSD2/nils/ocean_bench_exps/diffusion/data/million\", split=\"train\")\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\"Collate function for resizing images to the same size and normalization.\"\"\"\n",
    "    resize = K.Resize(size=(224, 224))\n",
    "    images = [resize(item[\"image\"].float()) for item in batch]\n",
    "    images = torch.stack(images) / 255.\n",
    "    return images\n",
    "\n",
    "# to easily generate a batch of images \n",
    "dl = DataLoader(ds, batch_size=16, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting Task\n",
    "\n",
    "For inpainting, some areas of the image are missing, and we use the diffusion model to fill (\"inpaint\") these areas to obtain a complete image. We will simulate this by creating masks so we can visualize the results.\n",
    "\n",
    "The implementation expects the following inputs:\n",
    "\n",
    "- images with applied mask to be inpainted\n",
    "- the mask itself (0 denotes missing pixels, 1 denotes pixels that can be used as context for inpainting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO inpainting code\n",
    "\n",
    "def plot_results(target, masked_gt, mask, inpainted):\n",
    "    \"\"\"Plot results.\n",
    "\n",
    "    Args:\n",
    "        target: full target\n",
    "        masked_gt: target with mask applied\n",
    "        mask: mask tensor\n",
    "        inpainted: inpainted tensor\n",
    "    \"\"\"\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    fig, axs = plt.subplots(batch_size, 4, figsize=(30, 5 * batch_size))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        target_np = target[i].numpy().transpose(1, 2, 0)\n",
    "        masked_gt_np = masked_gt[i].numpy().transpose(1, 2, 0)\n",
    "        mask_np = mask[i].numpy().transpose(1, 2, 0)\n",
    "        inpainted_np = inpainted[i].numpy().transpose(1, 2, 0)\n",
    "\n",
    "        axs[i, 0].imshow(target_np)\n",
    "        axs[i, 0].axis(\"off\")\n",
    "\n",
    "        axs[i, 1].imshow(masked_gt_np)\n",
    "        axs[i, 1].axis(\"off\")\n",
    "\n",
    "        axs[i, 2].imshow(mask_np, cmap=\"gray\")\n",
    "        axs[i, 2].axis(\"off\")\n",
    "\n",
    "        axs[i, 3].imshow(inpainted_np)\n",
    "        axs[i, 3].axis(\"off\")\n",
    "\n",
    "    axs[0, 0].set_title(\"Original Image\", fontsize=40)\n",
    "    axs[0, 1].set_title(\"Masked Input\", fontsize=40)\n",
    "    axs[0, 2].set_title(\"Mask\", fontsize=40)\n",
    "    axs[0, 3].set_title(\"Inpainted Image\", fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def create_center_square_mask(image_size: int, mask_size: int):\n",
    "    \"\"\"Create a mask that is a center square in the image.\"\"\"\n",
    "    assert image_size >= mask_size, \"Mask size should be smaller or equal to image size\"\n",
    "\n",
    "    mask = torch.zeros((image_size, image_size))\n",
    "    start = (image_size - mask_size) // 2\n",
    "    end = start + mask_size\n",
    "    mask[start:end, start:end] = 1\n",
    "\n",
    "    return (mask - 1) * -1\n",
    "\n",
    "def create_middle_column_mask(image_size: int, mask_size: int):\n",
    "    \"\"\"Create a mask that is a center column down the image\"\"\"\n",
    "    mask = torch.zeros((image_size, image_size))\n",
    "    start = (image_size - mask_size) // 2\n",
    "    end = start + mask_size\n",
    "    mask[:, start:end] = 1\n",
    "    return (mask - 1) * -1\n",
    "\n",
    "def prepare_data(imgs: Tensor) -> dict[str, Tensor]:\n",
    "    \"\"\"Prepare images for inpainting.\"\"\"\n",
    "    image_size = imgs.shape[-1]  \n",
    "    mask_size = image_size // 3 \n",
    "    masks = (\n",
    "        create_center_square_mask(image_size, mask_size)\n",
    "        .repeat(imgs.shape[0], 1, 1)\n",
    "        .unsqueeze(1)\n",
    "    )\n",
    "\n",
    "    # Apply the mask to the image\n",
    "    masked_imgs = imgs * masks\n",
    "\n",
    "    return {\n",
    "        \"image\": imgs,\n",
    "        \"mask\": masks,\n",
    "        \"masked_image\": masked_imgs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = next(iter(dl))\n",
    "\n",
    "data = prepare_data(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inpainting with UQ\n",
    "\n",
    "The diffusion model is stochastic meaning, that running the model multiple times for the same input will generate varying realizations. We can use this stochasticity as a notion of uncertainty for the inpainted regions. The code below will demonstrate how to do this and render some visualizations to give an intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples_with_uq(masked_gt, mask, samples, uncertainty, num_datapoints: int = 4, num_samples: int=5):\n",
    "    \"\"\"Plot random results.\n",
    "\n",
    "    Args:\n",
    "        masked_gt: target with mask applied tensor of shape [batch_size, C, H, W]\n",
    "        mask: mask tensor of shape [batch_size, 1, H, W]\n",
    "        samples: sample tensor of shape [batch_size, num_samples, C, H, W]\n",
    "        uncertainty: uncertainty tensor [batch_size, C, H, W]\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(masked_gt.size(0), size=num_datapoints, replace=False)\n",
    "    sample_indices = np.random.choice(samples.size(1), size=num_samples, replace=False)\n",
    "\n",
    "    fig, axs = plt.subplots(4, 2+num_samples, figsize=(60, 20))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        masked_gt_np = masked_gt[idx].numpy().transpose(1, 2, 0)\n",
    "        uncertainty_np = uncertainty[idx].numpy().transpose(1, 2, 0)\n",
    "\n",
    "        axs[i, 0].imshow(masked_gt_np)\n",
    "        axs[i, 0].axis(\"off\")\n",
    "\n",
    "        # plot the samples\n",
    "        for j, sample_idx in enumerate(sample_indices):\n",
    "            sample_np = samples[idx, sample_idx].numpy().transpose(1, 2, 0)\n",
    "            axs[i, j+1].imshow(sample_np)\n",
    "            axs[i, j+1].axis(\"off\")\n",
    "\n",
    "        axs[i, 7].imshow(uncertainty_np)\n",
    "        axs[i, 7].axis(\"off\")\n",
    "\n",
    "    \n",
    "    axs[0, 0].set_title(\"Masked Input\", fontsize=40)\n",
    "    for j in range(5):\n",
    "        axs[0, j+1].set_title(f\"Sample {j+1}\", fontsize=40)\n",
    "    axs[0, 6].set_title(\"Uncertainty\", fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UQ capabilities of diffusion models have been hightlighted in several publications, however, we believe there are many more interesting applications that could benefit from them. We hope that this tutorial potentially provided some ideas and insights that you might find useful for your tasks or research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testUQBox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
