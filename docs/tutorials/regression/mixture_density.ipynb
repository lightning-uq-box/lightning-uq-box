{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture Density Network 1D Regression\n",
    "\n",
    "This tutorial will showcase the application of a fairly \"old\" (in ML age) modeling technique called Mixture Density Networks that can be used for supervised regression problems, as well as inverse problems. For a very nice tutorial, with more mathematical details, see [this tutorial](https://github.com/dusenberrymw/mixture-density-networks/blob/master/mixture_density_networks.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install lightning-uq-box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from torch.optim import Adam\n",
    "\n",
    "from lightning_uq_box.datamodules import ToyHeteroscedasticDatamodule\n",
    "from lightning_uq_box.models import MLP\n",
    "from lightning_uq_box.uq_methods import DeterministicRegression, MDNRegression\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    "    plot_training_metrics,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 5]\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)  # seed everything for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a temporary directory to look at some training metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_temp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ToyHeteroscedasticDatamodule(n_points=500)\n",
    "\n",
    "X_train, Y_train, train_loader, X_test, Y_test, test_loader, X_gtext, Y_gtext = (\n",
    "    dm.X_train,\n",
    "    dm.Y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.Y_test,\n",
    "    dm.test_dataloader(),\n",
    "    dm.X_gtext,\n",
    "    dm.Y_gtext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_toy_regression_data(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Model\n",
    "\n",
    "For this toy regression problem, we will use a simple Mulit-layer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP(\n",
    "    n_inputs=1,\n",
    "    n_hidden=[50, 50, 50],\n",
    "    n_outputs=1,\n",
    "    dropout_p=0.1,\n",
    "    activation_fn=nn.Tanh(),\n",
    ")\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit deterministic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_model = DeterministicRegression(\n",
    "    model=network, loss_fn=nn.MSELoss(), optimizer=partial(Adam, lr=0.003)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=250,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=20,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    default_root_dir=my_temp_dir,\n",
    ")\n",
    "\n",
    "trainer.fit(det_model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_preds = det_model.predict_step(X_gtext)\n",
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_gtext,\n",
    "    Y_gtext,\n",
    "    deterministic_preds[\"pred\"].squeeze(-1),\n",
    "    title=\"Supervised Regression Deterministic Model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit deterministic model on inverse problem\n",
    "\n",
    "Okay, this is standard stuff, but what happens if we consider an inverse problem, where we swap X and Y and then solve for Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_inverse = ToyHeteroscedasticDatamodule(n_points=500, invert=True)\n",
    "\n",
    "X_train_inv, Y_train_inv, X_test_inv, Y_test_inv, X_gtext_inv, Y_gtext_inv = (\n",
    "    dm_inverse.X_train,\n",
    "    dm_inverse.Y_train,\n",
    "    dm_inverse.X_test,\n",
    "    dm_inverse.Y_test,\n",
    "    dm_inverse.X_gtext,\n",
    "    dm_inverse.Y_gtext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_toy_regression_data(X_train_inv, Y_train_inv, X_test_inv, Y_test_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP(\n",
    "    n_inputs=1,\n",
    "    n_hidden=[50, 50, 50],\n",
    "    n_outputs=1,\n",
    "    dropout_p=0.1,\n",
    "    activation_fn=nn.Tanh(),\n",
    ")\n",
    "det_inv_model = DeterministicRegression(\n",
    "    model=network, loss_fn=nn.MSELoss(), optimizer=partial(Adam, lr=0.003)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=250,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=20,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    default_root_dir=my_temp_dir,\n",
    ")\n",
    "\n",
    "trainer.fit(det_inv_model, dm_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_inv_preds = det_inv_model.predict_step(X_gtext_inv)\n",
    "fig = plot_predictions_regression(\n",
    "    X_train_inv,\n",
    "    Y_train_inv,\n",
    "    X_gtext_inv,\n",
    "    Y_gtext_inv,\n",
    "    det_inv_preds[\"pred\"].squeeze(-1),\n",
    "    title=\"Inverse Problem Deterministic Model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model has difficulties fitting the data, as it tries to find a deterministic mapping for this multi-modal dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Density Network Model\n",
    "\n",
    "The Mixture Density Network tackles the inverse problem by instead modeling the data as a mixture of multiple distributions, and effectively train a neural network to parameterize a Gaussian Mixture Model.\n",
    "\n",
    "For the inverse problem, we will again use a simple Mulit-layer Perceptron (MLP), but adapt it afterwards for the Mixture Density Network idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdn_temp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdn_base_network = MLP(\n",
    "    n_inputs=1,\n",
    "    n_hidden=[50, 50, 50],\n",
    "    n_outputs=1,\n",
    "    dropout_p=0.1,\n",
    "    activation_fn=nn.Tanh(),\n",
    ")\n",
    "mdn_base_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a model with a Mixture Density Layer as the last layer. This last layer will replace the last layer of the specified MLP architecture above, such that the method remains applicable for \"off-the-shelve\" models like found in [timm](https://github.com/huggingface/pytorch-image-models) as well as custom architectures of course. You can also only train the last layer and keep all other parameters frozen (via the `freeze_backbone` flag), which might be interesting for finetuning pretrained models to have a probabilistic output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdn_model = MDNRegression(\n",
    "    model=mdn_base_network, n_components=3, optimizer=partial(Adam, lr=0.003)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(mdn_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=250,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=20,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    default_root_dir=my_temp_dir,\n",
    ")\n",
    "\n",
    "trainer.fit(mdn_model, dm_inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a brief look at the loss to check whether the model has been trained to reasonable convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(mdn_temp_dir, \"lightning_logs\"), [\"train_loss\", \"val_loss\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Predictions\n",
    "\n",
    "Now we are ready for the \"big reveal\", namely whether we can fit the data more accurately with the Mixture Density approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_density_pred = mdn_model.predict_step(X_gtext_inv)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train_inv,\n",
    "    Y_train_inv,\n",
    "    X_gtext_inv,\n",
    "    Y_gtext_inv,\n",
    "    mixture_density_pred[\"pred\"],\n",
    "    aleatoric=mixture_density_pred[\"pred_uct\"],\n",
    "    title=\"Mixture Density Mean Predictions\",\n",
    "    show_bands=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "We can also draw conditional samples from the mixture distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_density_samples = mdn_model.sample(X_gtext_inv)\n",
    "\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train_inv,\n",
    "    Y_train_inv,\n",
    "    X_gtext_inv,\n",
    "    Y_gtext_inv,\n",
    "    mixture_density_samples,\n",
    "    title=\"Mixture Density Samples\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the Mixture Density Model was able to approximate $p(y|x)$ of the inverse problem reasonably well, much better than a default deterministic network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311uqbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
