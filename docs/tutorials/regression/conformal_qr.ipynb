{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58297a4",
   "metadata": {},
   "source": [
    "# Conformalized Quantile Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5519c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install lightning-uq-box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9bdb95",
   "metadata": {},
   "source": [
    "## Theoretic Foundation\n",
    "\n",
    "Conformalized Quantile Regression (CQR) is a post-hoc uncertainty quantification method to yield calibrated predictive uncertainty bands with proven coverage guarantees [Angelopoulos, 2021](https://people.eecs.berkeley.edu/~angelopoulos/publications/downloads/gentle_intro_conformal_dfuq.pdf). Based on a held out calibration set, CQR uses a score function to find a desired coverage quantile $\\hat{q}$ and conformalizes the QR output by adjusting the quantile bands via $\\hat{q}$ for an unseen test point as follows $x_{\\star}$:\n",
    "\n",
    "$$\n",
    "    T(x_{\\star})=[\\hat{y}_{\\alpha/2}(x_{\\star})-\\hat{q}, \\hat{y}_{1-\\alpha/2}(x_{\\star})+\\hat{q}]\n",
    "$$\n",
    "\n",
    "where $\\hat{y}_{\\alpha/2}(x_{\\star})$ is the lower quantile output and $\\hat{y}_{1-\\alpha/2}(x_{\\star})$, [Romano, 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf).\n",
    "\n",
    "**_NOTE:_** In the current setup we follow the Conformal Prediction Tutorial linked below, where they apply Conformal Prediction to Quantile Regression. However, Conformal Prediction is a more general statistical framework and model-agnostic in the sense that whatever Frequentist or Bayesian approach one might take, Conformal Prediction can be applied to calibrate your predictive uncertainty bands.\n",
    "\n",
    "Conformalize the quantile regression model. From Chapter 2.2 of [Angelopoulos, 2021](https://people.eecs.berkeley.edu/~angelopoulos/publications/downloads/gentle_intro_conformal_dfuq.pdf).\n",
    "\n",
    "1. Identify a heuristic notion of uncertainty for our problem and use it to construct a scoring function, where large scores imply a worse fit between inputs $X$ and scalar output $y$. In the regression case, it is recommended to use a quantile loss that is then adjusted for prediction time via the conformal prediction method.\n",
    "2. Compute the Score function output for all data points in your held out validation set and compute a desired quantile $\\hat{q}$ across those scores.\n",
    "3. Conformalize your quantile regression bands with the computed $\\hat{q}$. The prediction set for an unseen test point $x_{\\star}$ is now $T(x_{\\star})=[\\hat{y}_{\\alpha/2}(x_{\\star})-\\hat{q}, \\hat{y}_{1-\\alpha/2}(x_{\\star})+\\hat{q}]$ where $\\hat{y}_{\\alpha/2}(x_{\\star})$ is the lower quantile model output and $\\hat{y}_{1-\\alpha/2}(x_{\\star})$ is the upper quantile model output. This means we are adjusting the quantile bands symmetrically on either side and this conformalized prediction interval is guaranteed to satisfy our desired coverage requirement regardless of model choice.\n",
    "\n",
    "The central assumption conformal prediction makes is exchangeability, which restricts us in the following two ways [Barber, 2022](https://arxiv.org/abs/2202.13415):\n",
    "1. The order of samples does not change the joint probability\n",
    "   distribution. This is a more general notion to the commonly known i.i.d assumption, meaning that we assume that our data is independently drawn from the same distribution and does not change between the different data splits. In other words, we assume that any permutation of the dataset observations has equal probability.\n",
    "2. Our modeling procedure is assumed to handle our individual data samples symmetrically to ensure exchangeability beyond observed data. For example a sequence model that weighs more recent samples higher than older samples would violate this assumption, while a classical image classification model that is fixed to simply yield a prediction class for any input image does not. Exchangeability means that if I have all residuals of training and test data, I cannot say from looking at the residuals alone whether a residual belongs to a training or test point. A test point is equally likely to be any of the computed residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fb4454",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50eaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from lightning_uq_box.datamodules import ToyHeteroscedasticDatamodule\n",
    "from lightning_uq_box.models import MLP\n",
    "from lightning_uq_box.uq_methods import ConformalQR, QuantileRegression\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_calibration_uq_toolbox,\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    "    plot_training_metrics,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ce9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)  # seed everything for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019de19",
   "metadata": {},
   "source": [
    "We define a temporary directory to look at some training metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_temp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68d92eb",
   "metadata": {},
   "source": [
    "## Datamodule\n",
    "\n",
    "To demonstrate the method, we will make use of a Toy Regression Example that is defined as a [Lightning Datamodule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html). While this might seem like overkill for a small toy problem, we think it is more helpful how the individual pieces of the library fit together so you can train models on more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deed0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ToyHeteroscedasticDatamodule(n_points=1000, calib_fraction=0.6)\n",
    "\n",
    "(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    train_loader,\n",
    "    X_test,\n",
    "    Y_test,\n",
    "    test_loader,\n",
    "    X_calib,\n",
    "    Y_calib,\n",
    "    X_gtext,\n",
    "    Y_gtext,\n",
    ") = (\n",
    "    dm.X_train,\n",
    "    dm.Y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.Y_test,\n",
    "    dm.test_dataloader(),\n",
    "    dm.X_calib,\n",
    "    dm.Y_calib,\n",
    "    dm.X_gtext,\n",
    "    dm.Y_gtext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdabb35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_toy_regression_data(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820919c1",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For our Toy Regression problem, we will use a simple Multi-layer Perceptron (MLP) that you can configure to your needs. For the documentation of the MLP see [here](https://readthedocs.io/en/stable/api/models.html#MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = [0.1, 0.5, 0.9]\n",
    "network = MLP(\n",
    "    n_inputs=1, n_hidden=[50, 50, 50], n_outputs=len(quantiles), activation_fn=nn.Tanh()\n",
    ")\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791255bd",
   "metadata": {},
   "source": [
    "With an underlying neural network, we can now use our desired UQ-Method as a sort of wrapper. All UQ-Methods are implemented as [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html), so in this case we will use the Quantile Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2c6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_model = QuantileRegression(\n",
    "    model=network, optimizer=partial(torch.optim.Adam, lr=4e-3), quantiles=quantiles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041212f0",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "Now that we have a LightningDataModule and a UQ-Method as a LightningModule, we can conduct training with a [Lightning Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html). It has tons of options to make your life easier, so we encourage you to check the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=150,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    default_root_dir=my_temp_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9e90b",
   "metadata": {},
   "source": [
    "Training our model is now easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da8da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(qr_model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b4df5a",
   "metadata": {},
   "source": [
    "## Training Metrics\n",
    "\n",
    "To get some insights into how the training went, we can use the utility function to plot the training loss and RMSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fecc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6047bf",
   "metadata": {},
   "source": [
    "## Predictions without the Conformal Procedure\n",
    "\n",
    "The constructed Data Module contains two possible test variable. `X_test` are IID samples from the same noise distribution as the training data, while `X_gtext` (\"X ground truth extended\") are dense inputs from the underlying \"ground truth\" function without any noise that also extends the input range to either side, so we can visualize the method's UQ tendencies when extrapolating beyond the training data range. Thus, we will use `X_gtext` for visualization purposes, but use `X_test` to compute uncertainty and calibration metrics because we want to analyse how well the method has learned the noisy data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf9a34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_preds = qr_model.predict_step(X_gtext)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_gtext,\n",
    "    Y_gtext,\n",
    "    qr_preds[\"pred\"],\n",
    "    pred_quantiles=np.stack(\n",
    "        [qr_preds[\"lower_quant\"], qr_preds[\"pred\"].squeeze(), qr_preds[\"upper_quant\"]],\n",
    "        axis=-1,\n",
    "    ),\n",
    "    aleatoric=qr_preds[\"aleatoric_uct\"],\n",
    "    title=\"Quantile Regression\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_preds = qr_model.predict_step(X_test)\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    qr_preds[\"pred\"].cpu().numpy(),\n",
    "    qr_preds[\"pred_uct\"].numpy(),\n",
    "    Y_test.cpu().numpy(),\n",
    "    X_test.cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d7a9c",
   "metadata": {},
   "source": [
    "## Conformal Prediction\n",
    "\n",
    "As a calibration procedure we will apply Conformal Prediction now. The method is implemented in a LightningModule that will compute the $\\hat{q}$ value to adjust the quantile bands for subsequent predictions. The method allows us to define a theoretically guaranteed coverage error rate, which is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5768f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conformal_qr = ConformalQR(qr_model, quantiles=quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empiricial_coverage(targets, lower_quant, upper_quant):\n",
    "    \"\"\"Compute empirical coverage.\n",
    "\n",
    "    Args:\n",
    "        targets: targets\n",
    "        lower_quant: lower quantile\n",
    "        upper_quant: upper quantile\n",
    "\n",
    "    Returns:\n",
    "        empirical coverage\n",
    "    \"\"\"\n",
    "    targets = targets.squeeze()\n",
    "    lower_quant = lower_quant.squeeze()\n",
    "    upper_quant = upper_quant.squeeze()\n",
    "    emp_cov_bool = (targets >= lower_quant) & (targets <= upper_quant)\n",
    "    return torch.sum(emp_cov_bool).item() / len(emp_cov_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73069d1b",
   "metadata": {},
   "source": [
    "In the next step we are leveraging existing lightning functionality, by casting the post-hoc fitting procedure to obtain q_hat during a \"calibration\" procedure. For Conformal Prediction it is crucial to have a separate calibration set apart from any validation set you have for your experiment. The datamodule includes a `calibration_dataloader()` which we will use for the Conformal QR fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    default_root_dir=my_temp_dir,\n",
    "    max_epochs=1,\n",
    ")\n",
    "trainer.fit(conformal_qr, train_dataloaders=dm.calib_dataloader())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb42ca",
   "metadata": {},
   "source": [
    "Now we can make predictions for any new data points we are interested in or get conformalized predictions for the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e6ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(conformal_qr, dm)\n",
    "cqr_preds = conformal_qr.predict_step(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb033033",
   "metadata": {},
   "source": [
    "To test the results of the Conformal Procedure, we can compare the empirical coverage before and after. And we can see that indeed after Conformal QR the empirical coverage is at the desired rate of at least `1-alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5874f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_preds = qr_model.predict_step(X_test)\n",
    "cqr_preds = conformal_qr.predict_step(X_test)\n",
    "print(\n",
    "    \"Empirical Coverage before: \",\n",
    "    empiricial_coverage(Y_test, qr_preds[\"lower_quant\"], qr_preds[\"upper_quant\"]),\n",
    ")\n",
    "print(\n",
    "    \"Empirical Coverage after: \",\n",
    "    empiricial_coverage(Y_test, cqr_preds[\"lower_quant\"], cqr_preds[\"upper_quant\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d480b",
   "metadata": {},
   "source": [
    "## Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152f86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cqr_preds = conformal_qr.predict_step(X_gtext)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_gtext,\n",
    "    Y_gtext,\n",
    "    cqr_preds[\"pred\"].numpy(),\n",
    "    pred_quantiles=np.stack(\n",
    "        [\n",
    "            cqr_preds[\"lower_quant\"],\n",
    "            cqr_preds[\"pred\"].squeeze(-1),\n",
    "            cqr_preds[\"upper_quant\"],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    ),\n",
    "    aleatoric=cqr_preds[\"aleatoric_uct\"].numpy(),\n",
    "    title=\"Conformalized Quantile Regression\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90591bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cqr_preds = conformal_qr.predict_step(X_test)\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    cqr_preds[\"pred\"].cpu().numpy(),\n",
    "    cqr_preds[\"pred_uct\"].numpy(),\n",
    "    Y_test.cpu().numpy(),\n",
    "    X_test.cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb0312",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uqboxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
