{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Bayesian Last Layer (VBLL) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates the recently developed method called \"Variational Bayesian Last Layer\" from [Harrison et. al 2024](https://arxiv.org/abs/2404.11599) that introduces a sampling free Variational Inference method that can be attached as a last layer to a neural network. For more details we refer the reader to their paper or their [implementation](https://github.com/VectorInstitute/vbll) for which we provide a Lightning wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install git+https://github.com/lightning-uq-box/lightning-uq-box.git\n",
    "%pip install vbll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from lightning_uq_box.datamodules import ToyHeteroscedasticDatamodule\n",
    "from lightning_uq_box.models import MLP\n",
    "from lightning_uq_box.uq_methods import VBLLRegression\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    "    plot_training_metrics,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary directory for saving\n",
    "my_temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ToyHeteroscedasticDatamodule(batch_size=64)\n",
    "\n",
    "X_train, Y_train, train_loader, X_test, Y_test, test_loader, X_gtext, Y_gtext = (\n",
    "    dm.X_train,\n",
    "    dm.Y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.Y_test,\n",
    "    dm.test_dataloader(),\n",
    "    dm.X_gtext,\n",
    "    dm.Y_gtext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_toy_regression_data(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a backbone model, to which the VBLL layer will be attached. This can be any neural network architecture, also pretrained ones. For this regression task, we will use a simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP(n_inputs=1, n_hidden=[64, 64], n_outputs=64, activation_fn=nn.Tanh())\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the VBLL layer will be attached to the network to form a model that can be trained. If you have a pretrained model, there is a `freeze_backbone` argument, and if true, only the last variational layer will be trained.\n",
    "\n",
    "One should pay attention to the following hyperparameters:\n",
    "- `parameterization`: how the last layer covariance matrix is parameterized, possible options are `diagonal` or `dense`\n",
    "- `regularization_weight`: specifies how much the KL term should be regularized and will impact the epistemic uncertainty in the last layer. It should be 1 / (number of training examples) by default, however, it can also be used as a hyperparamter to tune the epistemic uncertainty, where larger regularization weight will lead to a larger epistemic uncertainty estimate\n",
    "- `prior_scale`: specifies the scale of the prior in the last layer\n",
    "- `wishart_scale`: specifies a regularizing weight of the noise covariance in the last layer and influences the aleatoric uncertainty\n",
    "\n",
    "Additionally, VBLL can also be applied to pretrained networks. For this purpose set `replace_ll=True` and `freeze_backbone=True` to replace the last layer in the architecture with a VBLL layer and only train this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbll_model = VBLLRegression(\n",
    "    model=network,\n",
    "    num_targets=1,\n",
    "    regularization_weight=(1 / X_train.shape[0]) * 50,\n",
    "    optimizer=partial(torch.optim.Adam, lr=3e-3),\n",
    "    parameterization=\"dense\",\n",
    "    prior_scale=1.0,\n",
    "    wishart_scale=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    accelerator=\"cpu\",\n",
    "    max_epochs=500,  # number of epochs we want to train\n",
    "    logger=logger,\n",
    "    log_every_n_steps=3,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    limit_val_batches=0.0,  # no validation runs\n",
    "    default_root_dir=my_temp_dir,\n",
    "    gradient_clip_val=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(vbll_model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Predictions\n",
    "\n",
    "The constructed Data Module contains two possible test variable. `X_test` are IID samples from the same noise distribution as the training data, while `X_gtext` (\"X ground truth extended\") are dense inputs from the underlying \"ground truth\" function without any noise that also extends the input range to either side, so we can visualize the method's UQ tendencies when extrapolating beyond the training data range. Thus, we will use `X_gtext` for visualization purposes, but use `X_test` to compute uncertainty and calibration metrics because we want to analyse how well the method has learned the noisy data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = vbll_model.predict_step(X_gtext)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_gtext,\n",
    "    Y_gtext,\n",
    "    preds[\"pred\"],\n",
    "    preds[\"pred_uct\"].squeeze(-1),\n",
    "    epistemic=preds[\"pred_uct\"].squeeze(-1),\n",
    "    show_bands=False,\n",
    "    title=\"VBLL\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
