{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0eca528",
   "metadata": {},
   "source": [
    "# Bayesian Neural Networks with Latent Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e02686",
   "metadata": {},
   "source": [
    "## Theoretic Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd23cdd",
   "metadata": {},
   "source": [
    "BNN with latent variables (LVs) extend BNNs with VI to encompass LVs that model aleatoric uncertainty. The BNN+LV model is proposed in [Depeweg, 2018](http://proceedings.mlr.press/v80/depeweg18a/depeweg18a.pdf).\n",
    "\n",
    "The likelihood is given by\n",
    "$$\n",
    "p(Y |\\theta,z ,X) =\n",
    "\\prod_{i=1}^K p(y_i|\\theta,z_i,x_i) =\n",
    "\\prod_{i=1}^K \\mathcal{N}(y_{i} | f_{\\theta}(x_i,z_i), \\Sigma).\n",
    "$$\n",
    "\n",
    "The prior on the weights is obtained as for BNNs with VI and is given by,\n",
    "\n",
    "$$\n",
    "    p(\\theta) = \\prod_{l=1}^L \\prod_{h=1}^{V_l }\\prod_{j=1}^{V_{l-1}+1} \\mathcal{N}(w_{hj, l} \\vert 0, \\lambda)\n",
    "$$\n",
    "where $w_{hj, l}$ is the h-th row and the j-th column of weight matrix $\\theta_L$ at layer index $L$ and $\\lambda$ is the prior variance. Note that as we use partially stochastic networks, the above may contain less factors $\\mathcal{N}(w_{hj, l} \\vert 0, \\lambda)$ depending on how many layers are stochastic. The prior distribution of the latent variables z is given by\n",
    "\n",
    "$$\n",
    "    p(z)=\\prod_{i=1}^K \\mathcal{N}(z_i \\vert 0, \\gamma)\n",
    "$$\n",
    "where $\\gamma$ is the prior variance.\n",
    "\n",
    "Then, with the assumed likelihood function and prior, a posterior over the\n",
    "weights $\\theta$ and latent variables $z$ is obtained via Bayes' rule:\n",
    "$$\n",
    "p(\\theta,z|\\mathcal{D}) = \\frac{p(Y|\\theta,z,X)p(\\theta)p(z)}{p(Y|X)\n",
    "$$\n",
    "\n",
    "The approximate the posterior is given by\n",
    "\n",
    "$$\n",
    "q(\\theta,z) =  \\underbrace{\\left[ \\prod_{l=1}^L\\! \\prod_{h=1}^{V_l}\\!  \\prod_{j=1}^{V_{l\\!-\\!1}\\!+\\!1} \\mathcal{N}(w_{hj,l}\\vert m^w_{hj,l},v^w_{hj,l})\\right]}_{\\text{\\small $q(\\theta)$}} \\times\n",
    "\\underbrace{\\left[\\prod_{i=1}^K \\mathcal{N}(z_i \\vert m_i^z, v_i^z) \\right]}_{\\text{\\small $q(\\mathbf{z})$}}.\n",
    "$$\n",
    "\n",
    "Now the parameters $m^w_{hj,l}$,$v^w_{hj,l}$ and $m^z_i$, $v^z_i$ can be obtained\n",
    "by minimizing a divergence between $p(\\theta, z| \\mathcal{D})$. Here the following approximation of the $\\alpha$ divergence, as proposed in [Lobato, 2016](http://proceedings.mlr.press/v48/hernandez-lobatob16.pdf) and [Depeweg, 2016](https://arxiv.org/abs/1605.07127), is used,\n",
    "\n",
    "\n",
    "$$\n",
    "E_\\alpha(q) = -\\log Z_q - \\frac{1}{\\alpha} \\sum_{n=1}^N\n",
    "\\log \\mathbf{E}_{\\Theta,z_n\\sim\\, q}\\left[ \\left( \\frac{p(\\mathbf{y}_n | \\Theta, \\mathbf{x}_n, z_n, \\mathbf{\\Sigma)}}\n",
    "{f(\\Theta)f_n(z_n)}\\right)^\\alpha \\right],\n",
    "$$\n",
    "\n",
    "where $Z_n$ is the normalising constant of the exponential form of approximate the posterior and $f(\\Theta)$ and $f_n(z_n)$ are functions depending on the parameters of the distributions of the prior on the weights and of the prior distribution of the latent variables, see [Depeweg, 2016](https://arxiv.org/abs/1605.07127) for details. In order to make this optimization problem scalable, SGD is used with mini-batches, and the expectation over $q$ is approximated with an average over $K$ samples drawn from $q$.\n",
    "\n",
    "The posterior predictive distribution is given by,\n",
    "$$\n",
    "    p(y_{\\star}\\vert x_{\\star}, \\mathcal{D}) = \\int  \\left[\\int \\mathcal{N}(y_{\\star} \\vert f_{\\theta}(x_{\\star}, z_{\\star}),  \\Sigma) \\mathcal{N}(z_{\\star} \\vert 0, \\gamma) dz_{\\star} \\right] p(\\theta, z \\vert \\mathcal{D}) d\\theta dz.\n",
    "$$\n",
    "\n",
    "The network prediction $f_{\\theta}(x_{\\star}, z_{\\star})$ uses $z_{\\star}$ sampled from the prior distribution $\\mathcal{N}(z_{\\star} \\vert 0, \\gamma)$ because this is the only evidence we have about the latent variable for a new data point since all data points are assumed to be independent. However, the above posterior predictive distribution is intractable in this form. So instead we use sampling from the posterior distribution of the weights. The mean prediction is then given by the mean prediction of samples and the predictive uncertainty is obtained as standard deviation of samples from the approximation to the posterior predictive distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110bc42d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33472f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from lightning_uq_box.datamodules import ToyHeteroscedasticDatamodule\n",
    "from lightning_uq_box.models import MLP\n",
    "from lightning_uq_box.uq_methods import BNN_LV_VI_Batched_Regression\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_calibration_uq_toolbox,\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    "    plot_training_metrics,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd8296",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)  # seed everything for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4311d9f0",
   "metadata": {},
   "source": [
    "We define a temporary directory to look at some training metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_temp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c225ef",
   "metadata": {},
   "source": [
    "## Datamodule\n",
    "\n",
    "To demonstrate the method, we will make use of a Toy Regression Example that is defined as a [Lightning Datamodule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html). While this might seem like overkill for a small toy problem, we think it is more helpful how the individual pieces of the library fit together so you can train models on more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a228128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ToyHeteroscedasticDatamodule(batch_size=50)\n",
    "\n",
    "X_train, y_train, train_loader, X_test, y_test, test_loader = (\n",
    "    dm.X_train,\n",
    "    dm.y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.y_test,\n",
    "    dm.test_dataloader(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a62462",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_toy_regression_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6e1312",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For our Toy Regression problem, we will use a simple Multi-layer Perceptron (MLP) that will be converted to a BNN as well as a Latent Variable Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa04ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn = MLP(n_inputs=1, n_hidden=[25, 25, 25, 25, 25], n_outputs=1, activation_fn=nn.ReLU())\n",
    "latent_net = MLP(\n",
    "    n_inputs=2,  # num_input_features + num_target_dim\n",
    "    n_outputs=2,  # 2 * lv_latent_dim\n",
    "    n_hidden=[20],\n",
    "    activation_fn=nn.ReLU(),\n",
    ")\n",
    "bnn, latent_net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4dd4c3",
   "metadata": {},
   "source": [
    "With an underlying neural network, we can now use our desired UQ-Method as a sort of wrapper. All UQ-Methods are implemented as [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) that allow us to concisely organize the code and remove as much boilerplate code as possible.\n",
    "\n",
    "The BNN_LV_VI_Batched_Regression model enables us to chose the layers of our network we want to make stochastic via the ```stochastic_module_names``` argument. This can be done by either a list of module names or a list of module numbers. For example, stochastic_module_name = [-1] would only make the last layer stochastic while all other layers remain determinstic.\n",
    "The default value of None makes all layers stochastic. The hyperparameter ```alpha``` determines how the Gaussian approximation of the posterior fits the posterior of the weights, see Figure 1, [Depeweg, 2016](https://arxiv.org/abs/1605.07127)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11298c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_vi_model = BNN_LV_VI_Batched_Regression(\n",
    "    bnn,\n",
    "    latent_net,\n",
    "    optimizer=partial(torch.optim.Adam, lr=1e-3),\n",
    "    num_training_points=X_train.shape[0],\n",
    "    n_mc_samples_train=10,\n",
    "    n_mc_samples_test=50,\n",
    "    output_noise_scale=1.3,\n",
    "    prior_mu=0.0,\n",
    "    prior_sigma=1.0,\n",
    "    posterior_mu_init=0.0,\n",
    "    posterior_rho_init=-6.0,\n",
    "    alpha=1e-6,\n",
    "    bayesian_layer_type=\"reparameterization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade66141",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "Now that we have a LightningDataModule and a UQ-Method as a LightningModule, we can conduct training with a [Lightning Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html). It has tons of options to make your life easier, so we encourage you to check the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=250,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    limit_val_batches=0.0,  # skip validation runs\n",
    "    default_root_dir=my_temp_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41912724",
   "metadata": {},
   "source": [
    "Training our model is now easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc310a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(bnn_vi_model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48685cf2",
   "metadata": {},
   "source": [
    "## Training Metrics\n",
    "\n",
    "To get some insights into how the training went, we can use the utility function to plot the training loss and RMSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758c686",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a172267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "trainer.test(bnn_vi_model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43a9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bnn_vi_model.predict_step(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b259826",
   "metadata": {},
   "source": [
    "## Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec79f640",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    preds[\"pred\"],\n",
    "    preds[\"pred_uct\"],\n",
    "    epistemic=preds[\"epistemic_uct\"],\n",
    "    aleatoric=preds[\"aleatoric_uct\"],\n",
    "    title=\"title here\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8193931",
   "metadata": {},
   "source": [
    "For some additional metrics relevant to UQ, we can use the great [uncertainty-toolbox](https://uncertainty-toolbox.github.io/) that gives us some insight into the calibration of our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0c00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_calibration_uq_toolbox(\n",
    "    preds[\"pred\"].numpy(),\n",
    "    preds[\"pred_uct\"].numpy(),\n",
    "    y_test.cpu().numpy(),\n",
    "    X_test.cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d2f6b6",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "Links to othere related literature that might be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de992d19",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uqboxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
