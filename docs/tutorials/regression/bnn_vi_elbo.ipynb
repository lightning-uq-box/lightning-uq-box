{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28da4af3",
   "metadata": {},
   "source": [
    "# Bayes By Backprop - Mean Field Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23287880",
   "metadata": {},
   "source": [
    "## Theoretic Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd39c2",
   "metadata": {},
   "source": [
    "\n",
    " Bayesian Neural Networks (BNNs) with variational inference (VI) are an approximate Bayesian method. Here we use the mean-field assumption meaning that the variational distribution can be factorized as a product of individual Gaussian distributions. This method maximizes the evidence lower bound (ELBO) via standard stochastic gradient descent by using the reparameterization trick [Kingma, 2013](https://arxiv.org/abs/1312.6114) to backpropagate through the necessary sampling procedure. This results in a diagonal Gaussian approximation of the posterior distribution over the model parameters.\n",
    "\n",
    "The predictive likelihood is given by,\n",
    "\n",
    "$$\n",
    "p(Y |\\theta ,X) =\n",
    "\\prod_{i=1}^K p(y_i|\\theta,x_i) =\n",
    "\\prod_{i=1}^K \\mathcal{N}(y_{i} | f_{\\theta}(x_i), \\Sigma).\n",
    "$$\n",
    "\n",
    "\n",
    "The prior on the weights is given by,\n",
    "\n",
    "$$\n",
    "    p(\\theta) = \\prod_{l=1}^L \\prod_{h=1}^{V_l }\\prod_{j=1}^{V_{l-1}+1} \\mathcal{N}(w_{hj, l} \\vert 0, \\lambda)\n",
    "$$\n",
    "where $w_{hj, l}$ is the h-th row and the j-th column of weight matrix $\\theta_L$ at layer index $L$ and $\\lambda$ is the prior variance. Note that as we use partially stochastic networks, the above may contain less factors $\\mathcal{N}(w_{hj, l} \\vert 0, \\lambda)$ depending on how many layers are stochastic. Then, the posterior distribution of the weights is obtained by Bayes' rule as\n",
    "\n",
    "$$\n",
    "    p(\\theta|\\mathcal{D}) = \\frac{p(Y |\\theta ,X) p(\\theta)}{p(Y | X)}.\n",
    "$$\n",
    "\n",
    "As the posterior distribution over the weights is intractable we use a variational approximation,\n",
    "\n",
    "$$\n",
    "    q(\\theta) \\approx p(\\theta|\\mathcal{D}),\n",
    "$$\n",
    "\n",
    "that is a diagonal Gaussian. Now given an input $x^{\\star}$, the predictive distribution can be obtained as\n",
    "\n",
    "$$\n",
    "    p(y^{\\star}|x^{\\star},\\mathcal{D}) = \\int p(y^{\\star} |\\theta , x^{\\star})  p(\\theta|\\mathcal{D}) d\\theta.\n",
    "$$\n",
    "\n",
    "As the above integral is intractable we approximate by sampling form the approximation $q(\\theta)$ to the posterior distribution of the weights. The weights are obtained by minimizing the evidence lower bound (ELBO) on the Kullback-Leibler (KL) divergence between the variational approximation and the posterior distribution over the weights. The ELBO is given by,\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(\\theta, (x^{\\star}, y^{\\star}) ) = \\beta D_{KL}(q(\\theta) || p(\\theta) ) + \\frac{1}{2}\\text{ln}\\left(2\\pi\\sigma^2\\right) + \\frac{1}{2\\sigma^2}\\left(f_{\\theta}(x^{\\star})-y^{\\star}\\right)^2 .\n",
    "$$\n",
    "\n",
    "The KL divergence can be computed analytically as both distributions are assumed to be diagonal Gaussians and the hyperparameter $\\beta$ can be used to weight the influence of the variational parameters relative to that of the data. The hyperparameter $\\sigma$ can be either fixed or set to be an additional output of the network.\n",
    "\n",
    "The predictive mean is obtained as the mean of the network output $f_{\\theta}$ with $S$ weight samples from the variational approximation $\\theta_s \\sim q(\\theta)$,\n",
    "\n",
    "$$\n",
    "     f_m(x^{\\star}) = \\frac{1}{S} \\sum_{s=1}^S  f_{\\theta_s}(x^{\\star}).\n",
    "$$\n",
    "\n",
    "The predictive uncertainty is given by the standard deviation thereof,\n",
    "\n",
    "$$\n",
    "    \\sigma_p(x^{\\star}) = \\sqrt{\\frac{1}{S} \\sum_{s=1}^S  \\left(f_{\\theta_s}(x^{\\star})-  f_m(x^{\\star}) \\right)^2}.\n",
    "$$\n",
    "\n",
    "If one uses the NLL and adapts the BNN to output a mean and standard deviation of a Gaussian $f_{\\theta_s}(x^{\\star}) = (\\mu_{\\theta_s}(x^{\\star}), \\sigma_{\\theta_s}(x^{\\star}))$, the mean prediction is given by\n",
    "\n",
    "$$\n",
    "    f_m(x^{\\star}) = \\frac{1}{S} \\sum_{s=1}^S  \\mu_{\\theta_s}(x^{\\star}).\n",
    "$$\n",
    "\n",
    "and we obtain the predictive uncertainty as the standard deviation of the corresponding Gaussian mixture model obtained by the weight samples,\n",
    "\n",
    "$$\n",
    "   \\sigma_p(x^{\\star}) = \\sqrt{\\frac{1}{S} \\sum_{s=1}^S  \\left(\\mu_{\\theta_s}(x^{\\star})-  f_m(x^{\\star}) \\right)^2+ \\sum_{s=1}^S \\sigma_{\\theta_s}^2(x^{\\star})}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da290e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lightning-uq-box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00baca",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f11736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from lightning_uq_box.datamodules import ToyHeteroscedasticDatamodule\n",
    "from lightning_uq_box.models import MLP\n",
    "from lightning_uq_box.uq_methods import NLL, BNN_VI_ELBO_Regression\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_calibration_uq_toolbox,\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    "    plot_training_metrics,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fb0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)  # seed everything for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4c0aa",
   "metadata": {},
   "source": [
    "We define a temporary directory to look at some training metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_temp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9739bb",
   "metadata": {},
   "source": [
    "## Datamodule\n",
    "\n",
    "To demonstrate the method, we will make use of a Toy Regression Example that is defined as a [Lightning Datamodule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html). While this might seem like overkill for a small toy problem, we think it is more helpful how the individual pieces of the library fit together so you can train models on more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf31680",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ToyHeteroscedasticDatamodule(batch_size=50)\n",
    "\n",
    "X_train, Y_train, train_loader, X_test, Y_test, test_loader, X_gtext, Y_gtext = (\n",
    "    dm.X_train,\n",
    "    dm.Y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.Y_test,\n",
    "    dm.test_dataloader(),\n",
    "    dm.X_gtext,\n",
    "    dm.Y_gtext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_toy_regression_data(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d3f95",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For our Toy Regression problem, we will use a simple Multi-layer Perceptron (MLP) that you can configure to your needs. For the documentation of the MLP see [here](https://torchgeo.readthedocs.io/en/stable/api/models.html#MLP). The following MLP will be converted to a BNN inside the LightningModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7379aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP(n_inputs=1, n_hidden=[50, 50], n_outputs=2, activation_fn=nn.Tanh())\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eefa5c",
   "metadata": {},
   "source": [
    "With an underlying neural network, we can now use our desired UQ-Method as a sort of wrapper. All UQ-Methods are implemented as [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) that allow us to concisely organize the code and remove as much boilerplate code as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c219c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbp_model = BNN_VI_ELBO_Regression(\n",
    "    network,\n",
    "    optimizer=partial(torch.optim.Adam, lr=1e-2),\n",
    "    criterion=NLL(),\n",
    "    stochastic_module_names=[-1],\n",
    "    num_mc_samples_train=10,\n",
    "    num_mc_samples_test=25,\n",
    "    burnin_epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f2873",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "Now that we have a LightningDataModule and a UQ-Method as a LightningModule, we can conduct training with a [Lightning Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html). It has tons of options to make your life easier, so we encourage you to check the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=20,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    default_root_dir=my_temp_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db6aab",
   "metadata": {},
   "source": [
    "Training our model is now easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cf97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(bbp_model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ba8bc",
   "metadata": {},
   "source": [
    "## Training Metrics\n",
    "\n",
    "To get some insights into how the training went, we can use the utility function to plot the training loss and RMSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e484e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b45c30",
   "metadata": {},
   "source": [
    "## Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bbp_model.predict_step(X_gtext)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_gtext,\n",
    "    Y_gtext,\n",
    "    preds[\"pred\"].squeeze(-1),\n",
    "    preds[\"pred_uct\"],\n",
    "    epistemic=preds[\"epistemic_uct\"],\n",
    "    aleatoric=preds[\"aleatoric_uct\"],\n",
    "    title=\"Bayes By Backprop MFVI\",\n",
    "    show_bands=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eaecee",
   "metadata": {},
   "source": [
    "For some additional metrics relevant to UQ, we can use the great [uncertainty-toolbox](https://uncertainty-toolbox.github.io/) that gives us some insight into the calibration of our prediction. For a discussion of why this is important, see ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bbp_model.predict_step(X_test)\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    preds[\"pred\"].cpu().numpy(),\n",
    "    preds[\"pred_uct\"].cpu().numpy(),\n",
    "    Y_test.cpu().numpy(),\n",
    "    X_test.cpu().numpy(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uqboxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
