{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28da4af3",
   "metadata": {},
   "source": [
    "# Bayes By Backprop - Mean Field Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23287880",
   "metadata": {},
   "source": [
    "## Theoretic Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffd39c2",
   "metadata": {},
   "source": [
    "\n",
    " Bayesian Neural Networks (BNNs) with variational inference (VI) are an approximate Bayesian method. Here we use the mean-field assumption meaning that the variational distribution can be factorized as a product of individual Gaussian distributions. This method maximizes the evidence lower bound (ELBO) via standard stochastic gradient descent by using the reparameterization trick [Kingma, 2013](https://arxiv.org/abs/1312.6114) to backpropagate through the necessary sampling procedure. This results in a diagonal Gaussian approximation of the posterior distribution over the model parameters.\n",
    "\n",
    "The predictive likelihood is given by,\n",
    "\n",
    "$$\n",
    "p(Y |\\theta ,X) =\n",
    "\\prod_{i=1}^K p(y_i|\\theta,x_i) =\n",
    "\\prod_{i=1}^K \\mathcal{N}(y_{i} | f_{\\theta}(x_i), \\Sigma).\n",
    "$$\n",
    "\n",
    "\n",
    "The prior on the weights is given by,\n",
    "\n",
    "$$\n",
    "    p(\\theta) = \\prod_{l=1}^L \\prod_{h=1}^{V_l }\\prod_{j=1}^{V_{l-1}+1} \\mathcal{N}(w_{hj, l} \\vert 0, \\lambda)\n",
    "$$\n",
    "where $w_{hj, l}$ is the h-th row and the j-th column of weight matrix $\\theta_L$ at layer index $L$ and $\\lambda$ is the prior variance. Note that as we use partially stochastic networks, the above may contain less factors $\\mathcal{N}(w_{hj, l} \\vert 0, \\lambda)$ depending on how many layers are stochastic. Then, the posterior distribution of the weights is obtained by Bayes' rule as\n",
    "\n",
    "$$\n",
    "    p(\\theta|\\mathcal{D}) = \\frac{p(Y |\\theta ,X) p(\\theta)}{p(Y | X)}.\n",
    "$$\n",
    "\n",
    "As the posterior distribution over the weights is intractable we use a variational approximation,\n",
    "\n",
    "$$\n",
    "    q(\\theta) \\approx p(\\theta|\\mathcal{D}),\n",
    "$$\n",
    "\n",
    "that is a diagonal Gaussian. Now given an input $x^{\\star}$, the predictive distribution can be obtained as\n",
    "\n",
    "$$\n",
    "    p(y^{\\star}|x^{\\star},\\mathcal{D}) = \\int p(y^{\\star} |\\theta , x^{\\star})  p(\\theta|\\mathcal{D}) d\\theta.\n",
    "$$\n",
    "\n",
    "As the above integral is intractable we approximate by sampling form the approximation $q(\\theta)$ to the posterior distribution of the weights. The weights are obtained by minimizing the evidence lower bound (ELBO) on the Kullback-Leibler (KL) divergence between the variational approximation and the posterior distribution over the weights. The ELBO is given by,\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(\\theta, (x^{\\star}, y^{\\star}) ) = \\beta D_{KL}(q(\\theta) || p(\\theta) ) + \\frac{1}{2}\\text{ln}\\left(2\\pi\\sigma^2\\right) + \\frac{1}{2\\sigma^2}\\left(f_{\\theta}(x^{\\star})-y^{\\star}\\right)^2 .\n",
    "$$\n",
    "\n",
    "The KL divergence can be computed analytically as both distributions are assumed to be diagonal Gaussians and the hyperparameter $\\beta$ can be used to weight the influence of the variational parameters relative to that of the data. The hyperparameter $\\sigma$ can be either fixed or set to be an additional output of the network.\n",
    "\n",
    "The predictive mean is obtained as the mean of the network output $f_{\\theta}$ with $S$ weight samples from the variational approximation $\\theta_s \\sim q(\\theta)$,\n",
    "\n",
    "$$\n",
    "     f_m(x^{\\star}) = \\frac{1}{S} \\sum_{s=1}^S  f_{\\theta_s}(x^{\\star}).\n",
    "$$\n",
    "\n",
    "The predictive uncertainty is given by the standard deviation thereof,\n",
    "\n",
    "$$\n",
    "    \\sigma_p(x^{\\star}) = \\sqrt{\\frac{1}{S} \\sum_{s=1}^S  \\left(f_{\\theta_s}(x^{\\star})-  f_m(x^{\\star}) \\right)^2}.\n",
    "$$\n",
    "\n",
    "If one uses the NLL and adapts the BNN to output a mean and standard deviation of a Gaussian $f_{\\theta_s}(x^{\\star}) = (\\mu_{\\theta_s}(x^{\\star}), \\sigma_{\\theta_s}(x^{\\star}))$, the mean prediction is given by\n",
    "\n",
    "$$\n",
    "    f_m(x^{\\star}) = \\frac{1}{S} \\sum_{s=1}^S  \\mu_{\\theta_s}(x^{\\star}).\n",
    "$$\n",
    "\n",
    "and we obtain the predictive uncertainty as the standard deviation of the corresponding Gaussian mixture model obtained by the weight samples,\n",
    "\n",
    "$$\n",
    "   \\sigma_p(x^{\\star}) = \\sqrt{\\frac{1}{S} \\sum_{s=1}^S  \\left(\\mu_{\\theta_s}(x^{\\star})-  f_m(x^{\\star}) \\right)^2+ \\sum_{s=1}^S \\sigma_{\\theta_s}^2(x^{\\star})}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da290e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install lightning-uq-box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00baca",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f11736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from lightning_uq_box.datamodules import ToyHeteroscedasticDatamodule\n",
    "from lightning_uq_box.models import MLP\n",
    "from lightning_uq_box.uq_methods import NLL, BNN_VI_ELBO_Regression\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_calibration_uq_toolbox,\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    "    plot_training_metrics,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837fb0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)  # seed everything for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d4c0aa",
   "metadata": {},
   "source": [
    "We define a temporary directory to look at some training metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_temp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9739bb",
   "metadata": {},
   "source": [
    "## Datamodule\n",
    "\n",
    "To demonstrate the method, we will make use of a Toy Regression Example that is defined as a [Lightning Datamodule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html). While this might seem like overkill for a small toy problem, we think it is more helpful how the individual pieces of the library fit together so you can train models on more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf31680",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ToyHeteroscedasticDatamodule(batch_size=32, n_points=500)\n",
    "\n",
    "X_train, Y_train, train_loader, X_test, Y_test, test_loader, X_gtext, Y_gtext = (\n",
    "    dm.X_train,\n",
    "    dm.Y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.Y_test,\n",
    "    dm.test_dataloader(),\n",
    "    dm.X_gtext,\n",
    "    dm.Y_gtext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_toy_regression_data(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d3f95",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For our Toy Regression problem, we will use a simple Multi-layer Perceptron (MLP) that you can configure to your needs. For the documentation of the MLP see [here](https://torchgeo.readthedocs.io/en/stable/api/models.html#MLP). The following MLP will be converted to a BNN inside the LightningModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7379aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP(n_inputs=1, n_hidden=[50, 50], n_outputs=2, activation_fn=nn.ReLU())\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eefa5c",
   "metadata": {},
   "source": [
    "With an underlying neural network, we can now use our desired UQ-Method as a sort of wrapper. All UQ-Methods are implemented as [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) that allow us to concisely organize the code and remove as much boilerplate code as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c219c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbp_model = BNN_VI_ELBO_Regression(\n",
    "    network,\n",
    "    optimizer=partial(torch.optim.Adam, lr=3e-3),\n",
    "    criterion=NLL(),\n",
    "    stochastic_module_names=[-1],\n",
    "    num_mc_samples_train=10,\n",
    "    num_mc_samples_test=25,\n",
    "    burnin_epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f2873",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "Now that we have a LightningDataModule and a UQ-Method as a LightningModule, we can conduct training with a [Lightning Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html). It has tons of options to make your life easier, so we encourage you to check the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a287d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=150,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=20,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    default_root_dir=my_temp_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db6aab",
   "metadata": {},
   "source": [
    "Training our model is now easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94cf97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(bbp_model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ba8bc",
   "metadata": {},
   "source": [
    "## Training Metrics\n",
    "\n",
    "To get some insights into how the training went, we can use the utility function to plot the training loss and RMSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e484e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b45c30",
   "metadata": {},
   "source": [
    "## Evaluate Predictions\n",
    "\n",
    "The constructed Data Module contains two possible test variable. `X_test` are IID samples from the same noise distribution as the training data, while `X_gtext` (\"X ground truth extended\") are dense inputs from the underlying \"ground truth\" function without any noise that also extends the input range to either side, so we can visualize the method's UQ tendencies when extrapolating beyond the training data range. Thus, we will use `X_gtext` for visualization purposes, but use `X_test` to compute uncertainty and calibration metrics because we want to analyse how well the method has learned the noisy data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bbp_model.predict_step(X_gtext)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_gtext,\n",
    "    Y_gtext,\n",
    "    preds[\"pred\"].squeeze(-1),\n",
    "    preds[\"pred_uct\"],\n",
    "    epistemic=preds[\"epistemic_uct\"],\n",
    "    aleatoric=preds[\"aleatoric_uct\"],\n",
    "    title=\"Bayes By Backprop MFVI\",\n",
    "    show_bands=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eaecee",
   "metadata": {},
   "source": [
    "For some additional metrics relevant to UQ, we can use the great [uncertainty-toolbox](https://uncertainty-toolbox.github.io/) that gives us some insight into the calibration of our prediction. For a discussion of why this is important, see ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789b4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bbp_model.predict_step(X_test)\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    preds[\"pred\"].cpu().numpy(),\n",
    "    preds[\"pred_uct\"].cpu().numpy(),\n",
    "    Y_test.cpu().numpy(),\n",
    "    X_test.cpu().numpy(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uqboxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
