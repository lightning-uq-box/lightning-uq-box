{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e1c9e7",
   "metadata": {},
   "source": [
    "# Gaussian Process Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba01ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/lightning-uq-box/lightning-uq-box.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab1ead",
   "metadata": {},
   "source": [
    "## Theoretic Foundation\n",
    "\n",
    " The goal of previously introduced methods was to find a distribution over the weights of a parameterized function i.e. a neural network. In contrast, the basic idea of a Gaussian Process (GP) is to instead consider a distribution over possible functions, that fit the data in some way. Formally,\n",
    "\n",
    "*\"A Gaussian process is a collection of random variables, any finite number of which have a joint Gaussian distribution.\"* [Seeger, 2004](https://www.worldscientific.com/doi/abs/10.1142/S0129065704001899)\n",
    "\n",
    "Precisely, a GP can be described by a possibly infinite amount of function values\n",
    "$$\n",
    "    f(x) \\sim \\mathcal{GP}(m(x),k_{\\gamma}(x)),\n",
    "$$\n",
    "\n",
    "such that any finite collection of function values $f$ has a joint Gaussian\n",
    "distribution,\n",
    "\n",
    "$$\n",
    " f = f(X) = [f(x_1),\\dots,f(x_K)]^{\\top} \\sim \\mathcal{N}(m_X,\\mathcal{K}_{X,X}),\n",
    "$$\n",
    "\n",
    "with a mean vector, $(m_X)i = m(x_i)$, and covariance matrix, $(\\mathcal{K}_{X,X})_{ij} = k_{\\gamma}(x_i,x_j)$,\n",
    "stemming from the mean function $m$ and covariance kernel of the GP, $k_{\\gamma}$, that is parametrized by $\\gamma$. A commonly used covariance function is the squared exponential, also referred to as Radial Basis Function (RBF) kernel, exponentiated quadratic or Gaussian kernel:\n",
    "\n",
    "$$\n",
    "    k_{\\gamma}(x,x') = \\text{cov}(f(x),f(x')) = \\eta^2\\exp{\\left(-\\frac{1}{2l^2}|x-x'|^2\\right)}.\n",
    "$$\n",
    "\n",
    "Where $\\gamma =( \\eta^2, l)$ and $\\eta^2$ can be set to $1$ or tuned as a hyperparameter. By default the lengthscale $l=1$ but can also be optimized over. Now the GP, $f (x) \\sim GP(m(x), k(x, x'))$, as a distribution over functions can be used to solve a regression problem. Following [Seeger, 2004](https://www.worldscientific.com/doi/abs/10.1142/S0129065704001899), consider the simple special case where the observations are noise free and you have training data $\\mathcal{D}_{\\mathrm{train}} = \\{(x_i, y_i)\\}_{i=1}^{K}$ with $X=(x_i)_{i=1}^K$ and $Y=(y_i)_{i=1}^K$. The joint prior distribution of the training outputs, $Y$, and the test outputs $f_*=f_*(X_*)= (f(x_k))_{k=1}^m$ where $X_* = (x_k)_{k=1}^m$ are the test points, according to the prior is\n",
    "\n",
    "$$\n",
    " p(Y,f_*) = \\mathcal{N}\\bigg(0, \\Bigg[ \\begin{array}{rr}\n",
    "\\mathcal{K}_{X,X} & \\mathcal{K}_{X,X_*}  \\\\\n",
    "\\mathcal{K}_{X_*,X} & \\mathcal{K}_{X_*,X_*}  \\\\\n",
    "\\end{array} \\Bigg] \\bigg).\n",
    "$$\n",
    "\n",
    "Here the mean function is assumed to be $m_X = 0$ and $\\mathcal{K}_{X,X_*}$ denotes the $K \\times m$ matrix of the covariances evaluated at all pairs of training and test\n",
    "points, and similarly for the other entries $\\mathcal{K}_{X,X}$, $\\mathcal{K}_{X_*,X_*}$ and $\\mathcal{K}_{X_*,X}$. To make predictions based on the knowledge of the training points, conditioning on the prior observations is used and yields,\n",
    "\n",
    "$$\n",
    "    p(f_*|X_*, X, Y) = \\mathcal{N}(\\mathcal{K}_{X_*,X}\\mathcal{K}_{X,X}^{-1}Y, \\mathcal{K}_{X_*,X_*}-\\mathcal{K}_{X_*,X}\\mathcal{K}_{X,X}^{-1}\\mathcal{K}_{X_*,X})\\\\\n",
    "    = \\mathcal{N}(m(X,X_*,Y), \\tilde{\\mathcal{K}}_{X,X_*}).\n",
    "$$\n",
    "\n",
    "Now to generate function values on test points, one uses samples from the posterior distribution $f_*(X_*) \\sim \\mathcal{N}(m(X,X_*,Y), \\tilde{K}(X, X_*))$.  To illustrate how we can obtain these samples from the posterior distribution, consider a Gaussian with arbitrary mean $m$ and covariance $K$, i.e. $f_* \\sim \\mathcal{N}(m,K)$. For this one can use a scalar Gaussian generator, which is available in many packages:\n",
    "\n",
    "- Compute the Cholesky decomposition of $K=LL^T$, where $L$ is a lower triangular matrix. This works because $K$ is symmetric by definition.\n",
    "- Then, draw multiple $u \\sim \\mathcal{N}(0, I)$.\n",
    "- Now, compute the samples with $f_* = m + Lu$. This has the desired mean, $m$ and covariance $L\\mathbb{E}(uu^T)L^T = LL^T = K$.\n",
    "\n",
    "\n",
    "The above can be extended to incorporate noisy measurements $y \\rightarrow y + e$, see [Seeger, 2004](https://www.worldscientific.com/doi/abs/10.1142/S0129065704001899), or noise on the inputs as in [Johnson, 2019](https://ieeexplore.ieee.org/abstract/document/8746634). Both of these extensions require tuning of further hyperparameters, yet beneficially allow to incorporate a prediction of aleatoric uncertainty in a GP.\n",
    "\n",
    "For example, assume additive Gaussian noise on the distribution of the function values,\n",
    "\n",
    "$$\n",
    "    p(y(x)|f(x)) =  \\mathcal{N}(y(x); f(x),\\sigma^2).\n",
    "$$\n",
    "\n",
    "Then the predictive distribution of the GP evaluated at the\n",
    "$K_*$ test points, $X_*$, is given by\n",
    "\n",
    "$$\n",
    " p(f_*|X_*,X,Y,\\gamma,\\sigma^2) = \\mathcal{N}(\\mathbb{E}[f_*],\\text{cov}(f_*)),\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\mathbb{E}[f_*] = m_{X_*}  + \\mathcal{K}_{X_*,X}[\\mathcal{K}_{X,X}+\\sigma^2 I]^{-1}Y,\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\text{cov}(f_*) = \\mathcal{K}_{X_*,X_*} - \\mathcal{K}_{X_*,X}[\\mathcal{K}_{X,X}+\\sigma^2 I]^{-1}\\mathcal{K}_{X,X_*}.\n",
    "$$\n",
    "\n",
    "Here $m_{X_*}$ is the $K_* \\times 1$ mean vector, which is assumed to be zero in the previous case.\n",
    "\n",
    "In both cases, with and without additive noise on the function values, the GP is trained by learning interpretable kernel hyperparameters. The log marginal likelihood of the targets $y$ - the probability of the data conditioned only on kernel hyperparameters $\\gamma$ - provides a principled probabilistic framework for kernel learning:\n",
    "$$\n",
    " \\log p(y | \\gamma, X) \\propto -\\left(y^{\\top}(\\mathcal{K}_{\\gamma}+\\sigma^2 I)^{-1}y + \\log|\\mathcal{K}_{\\gamma} + \\sigma^2 I|\\right),\n",
    "$$\n",
    "where $\\mathcal{K}_{\\gamma}$ is used for $\\mathcal{K}_{X,X}$ given $\\gamma$. Kernel learning can be achieved by optimizing the above with respect to $\\gamma$.\n",
    "\n",
    "The computational bottleneck for inference is solving the linear system\n",
    "$(\\mathcal{K}_{X,X}+\\sigma^2 I)^{-1}y$, and for kernel learning it is computing\n",
    "the log determinant $\\log|\\mathcal{K}_{X,X}+ \\sigma^2 I|$ in the marginal likelihood.\n",
    "The standard approach is to compute the Cholesky decomposition of the\n",
    "$K \\times K$ matrix $\\mathcal{K}_{X,X}$, which\n",
    "requires $\\mathcal{O}(K^3)$ operations and $\\mathcal{O}(K^2)$ storage.\n",
    "After inference is complete, the predictive mean costs $\\mathcal{O}(K)$,\n",
    "and the predictive variance costs $\\mathcal{O}(K^2)$, per test point\n",
    "$x_*$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4939b64f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51282b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from lightning.pytorch import seed_everything\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from lightning_uq_box.datamodules import ToyHeteroscedasticDatamodule\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_calibration_uq_toolbox,\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dca3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)  # seed everything for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d32835",
   "metadata": {},
   "source": [
    "We define a temporary directory to look at some training metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ebdf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_temp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08dc2c",
   "metadata": {},
   "source": [
    "## Datamodule\n",
    "\n",
    "To demonstrate the method, we will make use of a Toy Regression Example that is defined as a [Lightning Datamodule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html). While this might seem like overkill for a small toy problem, we think it is more helpful how the individual pieces of the library fit together so you can train models on more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b51cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ToyHeteroscedasticDatamodule()\n",
    "\n",
    "X_train, Y_train, train_loader, X_test, Y_test, test_loader, X_gtext, Y_gtext = (\n",
    "    dm.X_train,\n",
    "    dm.Y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.Y_test,\n",
    "    dm.test_dataloader(),\n",
    "    dm.X_gtext,\n",
    "    dm.Y_gtext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addd0b85",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig = plot_toy_regression_data(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06df45b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super().__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c280ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# init the GP model\n",
    "gp_model = ExactGPModel(X_train.squeeze(), Y_train.squeeze(), likelihood)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "gp_model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    gp_model.parameters(), lr=1e-2\n",
    ")  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "\n",
    "bar = tqdm(range(100))\n",
    "for i in bar:\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = gp_model(X_train.squeeze())\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, Y_train.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    bar.set_postfix(loss=f\"{loss.detach().cpu().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc877b",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c78bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    gp_preds = gp_model(X_gtext.cpu())\n",
    "\n",
    "gp_mean = gp_preds.mean.detach().cpu().numpy()\n",
    "gp_var = gp_preds.variance.detach().cpu().numpy()\n",
    "gp_covar = gp_preds.covariance_matrix.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5046cf",
   "metadata": {},
   "source": [
    "## Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d90ce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_gtext,\n",
    "    Y_gtext,\n",
    "    gp_mean[:, None],\n",
    "    np.sqrt(gp_var),\n",
    "    epistemic=np.sqrt(gp_var),\n",
    "    title=\"Gaussian Process\",\n",
    "    show_bands=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b86197",
   "metadata": {},
   "source": [
    "For some additional metrics relevant to UQ, we can use the great [uncertainty-toolbox](https://uncertainty-toolbox.github.io/) that gives us some insight into the calibration of our prediction. For a discussion of why this is important, see ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d317b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gp_preds = gp_model(X_test.cpu())\n",
    "\n",
    "gp_mean = gp_preds.mean.detach().cpu().numpy()\n",
    "gp_var = gp_preds.variance.detach().cpu().numpy()\n",
    "gp_covar = gp_preds.covariance_matrix.detach().cpu().numpy()\n",
    "\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    gp_mean, np.sqrt(gp_var), Y_test.cpu().numpy(), X_test.cpu().numpy()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uqboxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
