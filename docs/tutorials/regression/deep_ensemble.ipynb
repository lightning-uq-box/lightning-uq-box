{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5176e67",
   "metadata": {},
   "source": [
    "# Deep Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install git+https://github.com/lightning-uq-box/lightning-uq-box.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28e3577",
   "metadata": {},
   "source": [
    "## Theoretic Foundation\n",
    "\n",
    "Introduced in [Lakshminarayanan, 2017](https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf), Deep Ensembles approximate a posterior distribution over the model weights with a Gaussian mixture model over the output of separately initialized and trained networks. In [Wilson, 2020](https://proceedings.neurips.cc/paper/2020/file/322f62469c5e3c7dc3e58f5a4d1ea399-Paper.pdf) the authors showed that Deep Ensembles can be interpreted as a Bayesian method.\n",
    "\n",
    "For the Deep Ensembles model the predictive mean is given by the mean taken over $N \\in \\mathbb{N}$ models $f_{\\theta_i}(x^{\\star}) = \\mu_{\\theta_i}(x^{\\star})$ that output a mean with different weights $\\{\\theta_i\\}_{i=1}^N$,\n",
    "\n",
    "$$\n",
    "     \\mu(x^{\\star}) = \\frac{1}{N} \\sum_{i=1}^N  \\mu_{\\theta_i}(x^{\\star}).\n",
    "$$\n",
    "\n",
    "The predictive uncertainty is given by the standard deviation of the predictions of the $N$ different networks, Gaussian ensemble members,\n",
    "\n",
    "$$\n",
    "    \\sigma(x^{\\star}) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N  \\left(\\mu_{\\theta_i}(x^{\\star})-  \\mu(x^{\\star}) \\right)^2}.\n",
    "$$\n",
    "\n",
    "## Deep Ensembles GMM\n",
    "\n",
    "For the Deep Ensembles GMM model, the predictive mean is given by the mean taken over $N \\in \\mathbb{N}$ models $f_{\\theta_i}(x^{\\star}) = (\\mu_{\\theta_i}(x^{\\star}), \\sigma_{\\theta_i}(x^{\\star}))$ with different weights $\\{\\theta_i\\}_{i=1}^N$,\n",
    "\n",
    "$$\n",
    "     \\mu_g(x^{\\star}) = \\frac{1}{N} \\sum_{i=1}^N  \\mu_{\\theta_i}(x^{\\star}).\n",
    "$$\n",
    "\n",
    "The predictive uncertainty is given by the standard deviation of the Gaussian mixture model consisting of the $N$ different networks, Gaussian ensemble members,\n",
    "\n",
    "$$\n",
    "    \\sigma_g(x^{\\star}) = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N  \\left(\\mu_{\\theta_i}(x^{\\star})-  \\mu_g(x^{\\star}) \\right)^2+ \\frac{1}{N}  \\sum_{i=1}^N \\sigma_{\\theta_i}^2(x^\\star)}.\n",
    "$$\n",
    "\n",
    "Note that the difference between \"Deep Ensembles\" and \"Deep Ensembles GMM\" is that in the latter we also consider the predictive uncertainty output of each individual ensemble member, whereas in the former we only consider the means and the variance of the mean predictions of the ensemble members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a379cc",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "from lightning_uq_box.datamodules import ToyHeteroscedasticDatamodule\n",
    "from lightning_uq_box.models import MLP\n",
    "from lightning_uq_box.uq_methods import DeepEnsembleRegression, MVERegression\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_calibration_uq_toolbox,\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)  # seed everything for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec83c91",
   "metadata": {},
   "source": [
    "We define a temporary directory to look at some training metrics and results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df33eb72",
   "metadata": {},
   "source": [
    "## Datamodule\n",
    "\n",
    "To demonstrate the method, we will make use of a Toy Regression Example that is defined as a [Lightning Datamodule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html). While this might seem like overkill for a small toy problem, we think it is more helpful how the individual pieces of the library fit together so you can train models on more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cb13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_temp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75668d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ToyHeteroscedasticDatamodule()\n",
    "\n",
    "X_train, Y_train, train_loader, X_test, Y_test, test_loader, X_gtext, Y_gtext = (\n",
    "    dm.X_train,\n",
    "    dm.Y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.Y_test,\n",
    "    dm.test_dataloader(),\n",
    "    dm.X_gtext,\n",
    "    dm.Y_gtext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4537114",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_toy_regression_data(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d604dd",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For our Toy Regression problem, we will use a simple Multi-layer Perceptron (MLP) that you can configure to your needs. For the documentation of the MLP see [here](https://readthedocs.io/en/stable/api/models.html#MLP). In the case of the Deep Ensemble, we will train 5 differently initialzed base deterministic networks and later combine them into an ensemble for predictions. We will keep track of the model checkpoints of these models and save them manually in our temporary directory. Later we can use these to initialized the different members that make up our Ensemble during prediction, where only the corresponding ensemble member that is needed will be loaded to reduce memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ensembles = 5\n",
    "trained_models_nll = []\n",
    "for i in range(n_ensembles):\n",
    "    mlp_model = MLP(n_hidden=[50, 50], n_outputs=2, activation_fn=nn.Tanh())\n",
    "    ensemble_member = MVERegression(\n",
    "        mlp_model, optimizer=partial(torch.optim.Adam, lr=1e-2), burnin_epochs=20\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        max_epochs=150,\n",
    "        limit_val_batches=0,\n",
    "        num_sanity_val_steps=0,\n",
    "        logger=False,\n",
    "        enable_checkpointing=False,\n",
    "        default_root_dir=my_temp_dir,\n",
    "    )\n",
    "    trainer.fit(ensemble_member, dm)\n",
    "    save_path = os.path.join(my_temp_dir, f\"model_nll_{i}.ckpt\")\n",
    "    trainer.save_checkpoint(save_path)\n",
    "    trained_models_nll.append({\"base_model\": ensemble_member, \"ckpt_path\": save_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e400c6",
   "metadata": {},
   "source": [
    "## Construct Deep Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a96a55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_ens_nll = DeepEnsembleRegression(trained_models_nll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab7d2f9",
   "metadata": {},
   "source": [
    "## Evaluate Predictions\n",
    "\n",
    "The constructed Data Module contains two possible test variable. `X_test` are IID samples from the same noise distribution as the training data, while `X_gtext` (\"X ground truth extended\") are dense inputs from the underlying \"ground truth\" function without any noise that also extends the input range to either side, so we can visualize the method's UQ tendencies when extrapolating beyond the training data range. Thus, we will use `X_gtext` for visualization purposes, but use `X_test` to compute uncertainty and calibration metrics because we want to analyse how well the method has learned the noisy data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b6b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = deep_ens_nll.predict_step(X_gtext)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_gtext,\n",
    "    Y_gtext,\n",
    "    preds[\"pred\"],\n",
    "    preds[\"pred_uct\"],\n",
    "    epistemic=preds[\"epistemic_uct\"],\n",
    "    aleatoric=preds[\"aleatoric_uct\"],\n",
    "    title=\"Deep Ensemble NLL\",\n",
    "    show_bands=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10ebf0b",
   "metadata": {},
   "source": [
    "For some additional metrics relevant to UQ, we can use the great [uncertainty-toolbox](https://uncertainty-toolbox.github.io/) that gives us some insight into the calibration of our prediction. For a discussion of why this is important, see ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e0f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = deep_ens_nll.predict_step(X_test)\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    preds[\"pred\"].numpy(),\n",
    "    preds[\"pred_uct\"].numpy(),\n",
    "    Y_test.cpu().numpy(),\n",
    "    X_test.cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6da664",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "Links to othere related literature that might be interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ac89a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uqboxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
