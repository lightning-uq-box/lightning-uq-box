{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5f05d5",
   "metadata": {},
   "source": [
    "# Deep Evidential Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f10d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install git+https://github.com/lightning-uq-box/lightning-uq-box.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866e9ee5",
   "metadata": {},
   "source": [
    "## Theoretic Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854100a9",
   "metadata": {},
   "source": [
    "Deep Evidential Regression (DER) [Amini, 2020](https://proceedings.neurips.cc/paper/2020/file/aab085461de182608ee9f607f3f7d18f-Paper.pdf) is a single forward pass UQ method that aims to disentangle aleatoric and epistemic uncertainty.  DER entails a four headed network output\n",
    "\n",
    "$$\n",
    "    f_{\\theta}(x^{\\star})=(\\gamma_{\\theta}(x^{\\star}), \\nu_{\\theta}(x^{\\star}), \\alpha_{\\theta}(x^{\\star}), \\beta_{\\theta}(x^{\\star})),\n",
    "$$\n",
    "\n",
    "that is used to compute the predictive t-distribution with $2\\alpha(x^{\\star})$ degrees of freedom:\n",
    "\n",
    "$$\n",
    "    p(y(x^{\\star})|f_{\\theta}(x^{\\star}))=\\text{St}_{2\\alpha_{\\theta}(x^{\\star})}\\left(y^{\\star}\\bigg| \\gamma_{\\theta}(x^{\\star}), \\frac{\\beta_{\\theta}(x^{\\star})(1+\\nu_{\\theta}(x^{\\star}))}{\\nu_{\\theta}(x^{\\star})\\alpha_{\\theta}(x^{\\star})}\\right).\n",
    "$$\n",
    "\n",
    "In [Amini, 2020](https://proceedings.neurips.cc/paper/2020/file/aab085461de182608ee9f607f3f7d18f-Paper.pdf) the network weights are obtained by minimizing the loss objective that is the negative log-likelihood of the predictive distribution and a regularization term. However, due to several drawbacks of DER, \\cite{meinert2023unreasonable} propose the following adapted loss objective that we also utilise,\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}({\\theta},(x^{\\star}, y^{\\star}))=\\log\\sigma_{\\theta}^2(x^{\\star})+(1+\\lambda\\nu_{\\theta}(x^{\\star}))\\frac{(y^{\\star}-\\gamma_{\\theta}(x^{\\star}))^2}{\\sigma_{\\theta}^2(x^{\\star})}\n",
    "$$\n",
    "\n",
    "where $\\sigma_{\\theta}^2(x^{\\star})=\\beta_{\\theta}(x^{\\star})/\\nu_{\\theta}(x^{\\star})$. The mean prediction is given by\n",
    "\n",
    "$$\n",
    " \\mu_{\\theta}(x^{\\star}) = \\gamma_{\\theta}(x^{\\star})\n",
    "$$\n",
    "\n",
    "Further following [Meinert, 2022](https://ojs.aaai.org/index.php/AAAI/article/view/26096), we use their reformulation of the uncertainty decomposition. The aleatoric uncertainty is given by\n",
    "\n",
    "$$\n",
    "    u_{\\text{aleatoric}}(x^{\\star})=\\sqrt{\\frac{\\beta(x^{\\star})}{\\alpha(x^{\\star})-1}},\n",
    "$$\n",
    "\n",
    "and the epistemic uncertainy by,\n",
    "\n",
    "$$\n",
    "    u_{\\text{epistemic}}(x^{\\star})=\\frac{1}{\\sqrt{\\nu(x^{\\star})}}.\n",
    "$$\n",
    "\n",
    "The predictive uncertainty is then, given by\n",
    "\n",
    "$$\n",
    "    u(x^{\\star}) = \\sqrt{u_{\\text{epistemic}}(x^{\\star})^2+u_{\\text{aleatoric}}(x^{\\star})^2} .\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffe18a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f630e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from lightning_uq_box.datamodules import ToyHeteroscedasticDatamodule\n",
    "from lightning_uq_box.models import MLP\n",
    "from lightning_uq_box.uq_methods import DER\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_calibration_uq_toolbox,\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    "    plot_training_metrics,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9978c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)  # seed everything for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26030714",
   "metadata": {},
   "source": [
    "We define a temporary directory to look at some training metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eb6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_temp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8fde3b",
   "metadata": {},
   "source": [
    "## Datamodule\n",
    "\n",
    "To demonstrate the method, we will make use of a Toy Regression Example that is defined as a [Lightning Datamodule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html). While this might seem like overkill for a small toy problem, we think it is more helpful how the individual pieces of the library fit together so you can train models on more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d82d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ToyHeteroscedasticDatamodule()\n",
    "\n",
    "X_train, Y_train, train_loader, X_test, Y_test, test_loader, X_gtext, Y_gtext = (\n",
    "    dm.X_train,\n",
    "    dm.Y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.Y_test,\n",
    "    dm.test_dataloader(),\n",
    "    dm.X_gtext,\n",
    "    dm.Y_gtext,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748724d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_toy_regression_data(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b29c6a4",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For our Toy Regression problem, we will use a simple Multi-layer Perceptron (MLP) that you can configure to your needs. For the documentation of the MLP see [here](https://readthedocs.io/en/stable/api/models.html#MLP). For the DER Model our underlying network will require 4 outputs that feed into a DERLayer based on which the loss function will be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP(n_inputs=1, n_hidden=[50, 50, 50], n_outputs=4, activation_fn=nn.Tanh())\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e364a5e3",
   "metadata": {},
   "source": [
    "With an underlying neural network, we can now use our desired UQ-Method as a sort of wrapper. All UQ-Methods are implemented as [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) that allow us to concisely organize the code and remove as much boilerplate code as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d21f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "der_model = DER(network, optimizer=partial(torch.optim.Adam, lr=1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6858f7",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "Now that we have a LightningDataModule and a UQ-Method as a LightningModule, we can conduct training with a [Lightning Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html). It has tons of options to make your life easier, so we encourage you to check the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=250,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    "    default_root_dir=my_temp_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2924b",
   "metadata": {},
   "source": [
    "Training our model is now easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(der_model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87508ed5",
   "metadata": {},
   "source": [
    "## Training Metrics\n",
    "\n",
    "To get some insights into how the training went, we can use the utility function to plot the training loss and RMSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c25bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(\n",
    "    os.path.join(my_temp_dir, \"lightning_logs\"), [\"train_loss\", \"trainRMSE\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c068fd18",
   "metadata": {},
   "source": [
    "## Evaluate Predictions\n",
    "\n",
    "The constructed Data Module contains two possible test variable. `X_test` are IID samples from the same noise distribution as the training data, while `X_gtext` (\"X ground truth extended\") are dense inputs from the underlying \"ground truth\" function without any noise that also extends the input range to either side, so we can visualize the method's UQ tendencies when extrapolating beyond the training data range. Thus, we will use `X_gtext` for visualization purposes, but use `X_test` to compute uncertainty and calibration metrics because we want to analyse how well the method has learned the noisy data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = der_model.predict_step(X_gtext)\n",
    "\n",
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    X_gtext,\n",
    "    Y_gtext,\n",
    "    preds[\"pred\"],\n",
    "    preds[\"pred_uct\"].squeeze(-1),\n",
    "    epistemic=preds[\"epistemic_uct\"],\n",
    "    aleatoric=preds[\"aleatoric_uct\"],\n",
    "    title=\"Deep Evidential Regression\",\n",
    "    show_bands=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c697f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = der_model.predict_step(X_test)\n",
    "fig = plot_calibration_uq_toolbox(\n",
    "    preds[\"pred\"].squeeze(-1).cpu().numpy(),\n",
    "    preds[\"pred_uct\"].numpy(),\n",
    "    Y_test.cpu().numpy(),\n",
    "    X_test.cpu().numpy(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a85413",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uqboxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
