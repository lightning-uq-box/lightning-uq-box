{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254742fe",
   "metadata": {},
   "source": [
    "# Mean Variance Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c121626",
   "metadata": {},
   "source": [
    "## Theoretic Foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3346263f",
   "metadata": {},
   "source": [
    "The Gaussian model, also referred to as Mean Variance Estimation, was first studied in [Nix, 1994](https://ieeexplore.ieee.org/abstract/document/374138) and further used in [Sluijterman 2023](https://arxiv.org/abs/2302.08875), this is a deterministic model that predicts the parameters of a Gaussian distribution\n",
    "\n",
    "$$\n",
    "    f_{\\theta}(x^{\\star}) = (\\mu_{\\theta}(x^\\star),\\sigma_{\\theta}(x^\\star))\n",
    "$$\n",
    "\n",
    "in a single forward pass, where standard deviations $\\sigma_{\\theta}(x^\\star)$ can be used as a measure of data uncertainty. To this end the network now outputs two parameters and is trained with the Gaussian negative log-likelihood (NLL) as a loss objective [Kendall, 2017](https://proceedings.neurips.cc/paper_files/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf), that is given by\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(\\theta, (x^{\\star}, y^{\\star})) = \\frac{1}{2}\\text{ln}\\left(2\\pi\\sigma_{\\theta}(x^{\\star})^2\\right) + \\frac{1}{2\\sigma_{\\theta}(x^{\\star})^2}\\left(\\mu_{\\theta}(x^{\\star})-y^{\\star}\\right)^2.\n",
    "$$\n",
    "\n",
    "Correspondingly, the model prediction consists of a predictive mean, $\\mu_{\\theta}(x^\\star)$, and the predictive uncertainty - the parameters of a Gaussian distribution - in this case the standard deviation $\\sigma_{\\theta}(x^\\star)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11097b",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee7266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from lightning import Trainer\n",
    "from lightning.pytorch import seed_everything\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from lightning_uq_box.datamodules import ToyHeteroscedasticDatamodule\n",
    "from lightning_uq_box.models import MLP\n",
    "from lightning_uq_box.uq_methods import MVERegression\n",
    "from lightning_uq_box.viz_utils import (\n",
    "    plot_calibration_uq_toolbox,\n",
    "    plot_predictions_regression,\n",
    "    plot_toy_regression_data,\n",
    "    plot_training_metrics,\n",
    ")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [14, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f917d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(0)  # seed everything for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521507b9",
   "metadata": {},
   "source": [
    "We define a temporary directory to look at some training metrics and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab792707",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_temp_dir = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6dc79",
   "metadata": {},
   "source": [
    "## Datamodule\n",
    "\n",
    "To demonstrate the method, we will make use of a Toy Regression Example that is defined as a [Lightning Datamodule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html). While this might seem like overkill for a small toy problem, we think it is more helpful how the individual pieces of the library fit together so you can train models on more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9552698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = ToyHeteroscedasticDatamodule()\n",
    "\n",
    "X_train, y_train, train_loader, X_test, y_test, test_loader = (\n",
    "    dm.X_train,\n",
    "    dm.y_train,\n",
    "    dm.train_dataloader(),\n",
    "    dm.X_test,\n",
    "    dm.y_test,\n",
    "    dm.test_dataloader(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a809b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_toy_regression_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfaa240",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For our Toy Regression problem, we will use a simple Multi-layer Perceptron (MLP) that you can configure to your needs. For the documentation of the MLP see [here](https://readthedocs.io/en/stable/api/models.html#MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f97d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP(n_inputs=1, n_hidden=[50, 50, 50], n_outputs=2, activation_fn=nn.Tanh())\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce68a9c",
   "metadata": {},
   "source": [
    "With an underlying neural network, we can now use our desired UQ-Method as a sort of wrapper. All UQ-Methods are implemented as [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) that allow us to concisely organize the code and remove as much boilerplate code as possible. \n",
    "\n",
    "We can first train with the MSE loss, and only adapt the parameters of the mean prediction for the number of ```burnin_eochs``` for more stable training, before switching to the NLL loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f165f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mve_model = MVERegression(\n",
    "    model=network, optimizer=partial(torch.optim.Adam, lr=1e-2), burnin_epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fafb48c",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "Now that we have a LightningDataModule and a UQ-Method as a LightningModule, we can conduct training with a [Lightning Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html). It has tons of options to make your life easier, so we encourage you to check the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac323e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = CSVLogger(my_temp_dir)\n",
    "trainer = Trainer(\n",
    "    max_epochs=300,  # number of epochs we want to train\n",
    "    logger=logger,  # log training metrics for later evaluation\n",
    "    log_every_n_steps=1,\n",
    "    enable_checkpointing=False,\n",
    "    enable_progress_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa5df2",
   "metadata": {},
   "source": [
    "Training our model is now easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d68c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(mve_model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca2617",
   "metadata": {},
   "source": [
    "## Training Metrics\n",
    "\n",
    "To get some insights into how the training went, we can use the utility function to plot the training loss and RMSE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c001727",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_training_metrics(os.path.join(my_temp_dir, \"lightning_logs\"), \"RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a680c83",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "For prediction we can either rely on the `trainer.test()` method or manually conduct a `predict_step()`. Using the trainer will save the predictions and some metrics to a CSV file, while the manual `predict_step()` with a single input tensor will generate a dictionary that holds the mean prediction as well as some other quantities of interest, for example the predicted standard deviation or quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = mve_model.predict_step(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265497a",
   "metadata": {},
   "source": [
    "## Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56245f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_predictions_regression(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    preds[\"pred\"],\n",
    "    preds[\"pred_uct\"],\n",
    "    aleatoric=preds[\"aleatoric_uct\"],\n",
    "    title=\"Mean Variance Estimation Network\",\n",
    "    show_bands=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2c8587",
   "metadata": {},
   "source": [
    "For some additional metrics relevant to UQ, we can use the great [uncertainty-toolbox](https://uncertainty-toolbox.github.io/) that gives us some insight into the calibration of our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_calibration_uq_toolbox(\n",
    "    preds[\"pred\"].cpu().numpy(),\n",
    "    preds[\"pred_uct\"].cpu().numpy(),\n",
    "    y_test.cpu().numpy(),\n",
    "    X_test.cpu().numpy(),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uqboxEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
